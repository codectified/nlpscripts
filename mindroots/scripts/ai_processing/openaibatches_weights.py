import os
import csv
import json
import time
from openai import OpenAI
from dotenv import load_dotenv

# â”€â”€â”€ Environment & Client Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Load API credentials and initialize the OpenAI client. This keeps secrets
# out of source control and centralizes configuration.
load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OpenAI API key is missing. Set it in your .env file or environment variables.")
client = OpenAI(api_key=api_key)

def get_batch_indexes(directory=".", input_prefix="batchinput_", output_prefix="batch_output_", suffix=".jsonl"):
    input_batches = set()
    output_batches = set()

    for filename in os.listdir(directory):
        if filename.startswith(input_prefix) and filename.endswith(suffix):
            try:
                batch_num = int(filename[len(input_prefix):-len(suffix)])
                input_batches.add(batch_num)
            except ValueError:
                continue
        elif filename.startswith(output_prefix) and filename.endswith(suffix):
            try:
                batch_num = int(filename[len(output_prefix):-len(suffix)])
                output_batches.add(batch_num)
            except ValueError:
                continue
    return input_batches, output_batches


# â”€â”€â”€ Step 1: Ingest Word Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_csv(file_path, limit_definition_length=500):
    """
    Read raw word entries from a CSV and truncate definitions.
    Returns a list of tuples: (entry_id, arabic_word, truncated_definition).
    """
    words_data = []
    with open(file_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            entry_id  = row['entry_id_xml']       # carry your real node ID
            word      = row['word']
            definition = row['definitions_xml']
            if len(definition) > limit_definition_length:
                definition = definition[:limit_definition_length] + "..."
            words_data.append((entry_id, word, definition))
    return words_data

# â”€â”€â”€ Step 2: Prepare LLM Batch Requests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json

def create_batch_input_file(words_data, file_name="batchinput.jsonl", start_idx=0, batch_size=1000):
    system_prompt = (
        "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ. Ø£Ø¹Ø·Ù Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø¹Ø·Ø§Ø© ÙÙ‚Ø· Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„ØŒ "
        "Ù…Ø¹ ØªÙ…Ø«ÙŠÙ„Ù‡ Ø£ÙŠØ¶Ù‹Ø§ ÙÙŠ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ù…ÙˆØ²:\n"
        " - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù‚Ù… 3 Ù„Ø­Ø±Ù Ø¹\n"
        " - Ø§Ø³ØªØ®Ø¯Ù… 33 Ù„ØªÙƒØ±Ø§Ø± Ø¹ Ù…Ø¹ Ø§Ù„Ø´Ø¯Ø©\n\n"
        "ÙŠØ¬Ø¨ Ø£Ù† ØªÙØ±Ø¬Ø¹ ÙÙ‚Ø· ÙƒØ§Ø¦Ù† JSON Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:\n"
        "{\n"
        "  \"id\": \"<Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ø±Ù>\",\n"
        "  \"wazn\": \"<ÙˆØ²Ù† ÙƒØ§Ù…Ù„ Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„>\",\n"
        "  \"form\": \"<transliteration>\"\n"
        "}\n\n"
        "Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ ÙˆØ²Ù† Ù…Ø¹Ø±ÙˆÙØŒ Ø§Ø³ØªØ®Ø¯Ù…:\n"
        "{\"id\": \"<Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ø±Ù>\", \"wazn\": \"NA\", \"form\": \"NA\"}\n\n"
        "**Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ø¶Ø­Ø©:**\n"
        "{\"id\":\"001\",\"wazn\":\"ÙÙØ¹ÙÙ„Ù\",\"form\":\"fa3ala\"} â† ÙƒÙØªÙØ¨Ù\n"
        "{\"id\":\"002\",\"wazn\":\"ÙÙØ¹Ù‘ÙÙ„Ù\",\"form\":\"fa33ala\"} â† Ø¹ÙÙ„Ù‘ÙÙ…Ù\n"
        "{\"id\":\"003\",\"wazn\":\"Ø£ÙÙÙ’Ø¹ÙÙ„Ù\",\"form\":\"af3ala\"} â† Ø£ÙÙƒÙ’Ø±ÙÙ…Ù\n"
        "{\"id\":\"004\",\"wazn\":\"ÙÙØ§Ø¹ÙÙ„Ù\",\"form\":\"fÄ3ala\"} â† ØµÙØ§Ø­ÙØ¨Ù\n"
        "{\"id\":\"005\",\"wazn\":\"Ù…ÙÙÙ’Ø¹ÙÙ„ÙŒ\",\"form\":\"muf3alun\"} â† Ù…ÙØ¬Ù’ØªÙÙ…ÙØ¹ÙŒ\n"
        "{\"id\":\"006\",\"wazn\":\"ÙÙØ¹Ù‘ÙØ§Ù„\",\"form\":\"fa33Äl\"} â† ÙÙØªÙ‘ÙØ§Ø­\n"
        "{\"id\":\"007\",\"wazn\":\"Ù…ÙÙÙ’Ø¹ÙØ§Ù„\",\"form\":\"mif3Äl\"} â† Ù…ÙÙ‚Ù’Ø¯ÙØ§Ù…\n"
        "{\"id\":\"008\",\"wazn\":\"ÙÙØ¹ÙÙˆÙ„\",\"form\":\"fa3Å«l\"} â† Ø´ÙÙƒÙÙˆØ±\n"
        "{\"id\":\"009\",\"wazn\":\"ÙÙØ¹ÙŠÙ„\",\"form\":\"fa3Ä«l\"} â† Ø¹ÙÙ„ÙÙŠÙ…\n"
        "{\"id\":\"010\",\"wazn\":\"ÙÙØ¹ÙÙ„\",\"form\":\"fa3il\"} â† Ø­ÙØ°ÙØ±\n"
        "{\"id\":\"011\",\"wazn\":\"ÙÙØ¹ÙØ§Ù„ÙŒ\",\"form\":\"fi3Äl\"} â† ÙƒÙØªÙØ§Ø¨ÙŒ\n"
        "{\"id\":\"012\",\"wazn\":\"ÙÙØ¹ÙÙ„ÙŒ\",\"form\":\"fu3ul\"} â† ÙƒÙØªÙØ¨ÙŒ\n"
        "{\"id\":\"013\",\"wazn\":\"ÙÙØ¹Ù‘ÙØ§Ù„\",\"form\":\"fu33Äl\"} â† ÙƒÙØªÙ‘ÙØ§Ø¨\n"
        "{\"id\":\"014\",\"wazn\":\"ÙÙØ¹ÙŠÙ„ÙŒ\",\"form\":\"fa3Ä«lun\"} â† Ø³ÙÙ…ÙÙŠØ¹ÙŒ\n"
        "{\"id\":\"015\",\"wazn\":\"ÙÙØ¹ÙÙˆÙ„ÙŒ\",\"form\":\"fa3Å«lun\"} â† Ø±ÙØ³ÙÙˆÙ„ÙŒ\n"
        "{\"id\":\"016\",\"wazn\":\"ÙÙØ¹Ù’Ù„ÙØ©ÙŒ\",\"form\":\"fa3lah\"} â† Ø¬ÙÙ„Ù’Ø³ÙØ©ÙŒ\n"
        "{\"id\":\"017\",\"wazn\":\"Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙŒ\",\"form\":\"maf3Å«lun\"} â† Ù…ÙÙ‚Ù’Ø¨ÙÙˆÙ„ÙŒ\n"
        "{\"id\":\"018\",\"wazn\":\"Ù…ÙÙÙ’Ø¹ÙÙ„ÙŒ\",\"form\":\"mif3al\"} â† Ù…ÙÙÙ’ØªÙØ§Ø­ÙŒ\n"
        "{\"id\":\"019\",\"wazn\":\"ÙÙØ¹Ù‘ÙØ§Ù„ÙØ©ÙŒ\",\"form\":\"fa33Älah\"} â† Ø·ÙØ¨Ù‘ÙØ§Ø®ÙØ©ÙŒ\n\n"
        "Ù„Ø§ ØªÙØ±Ø¬ÙØ¹ Ø£ÙŠ Ø´Ø±Ø­. ÙÙ‚Ø· ÙƒØ§Ø¦Ù† JSON."
    )

    with open(file_name, 'w', encoding='utf-8') as f:
        for entry_id, word, definition in words_data[start_idx:start_idx + batch_size]:
            batch_request = {
                "custom_id": entry_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": "gpt-4o-mini",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f'ID: "{entry_id}"\nWord: "{word}"'}
                    ]
                }
            }
            f.write(json.dumps(batch_request, ensure_ascii=False) + "\n")

    print(f"âœ… Batch input file '{file_name}' created with {min(batch_size, len(words_data) - start_idx)} entries.")

# â”€â”€â”€ Step 3: Push Batch File to OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upload_batch_file(file_name="batchinput.jsonl"):
    """
    Upload the prepared JSONL as an OpenAI File resource for batch processing.
    Abstractly: stage LLM payloads on the server â†’ returns file handle.
    """
    with open(file_name, 'rb') as f:
        batch_input_file = client.files.create(file=f, purpose="batch")
    print(f"File '{file_name}' uploaded with ID {batch_input_file.id}")
    return batch_input_file.id


# â”€â”€â”€ Step 4: Kick Off the Batch Job â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_batch(batch_input_file_id):
    """
    Instruct OpenAI to execute the batch of chat completions.
    Abstractly: schedule asynchronous LLM work â†’ returns batch job handle.
    """
    batch = client.batches.create(
        input_file_id=batch_input_file_id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={"description": "Processing Arabic words batch"}
    )
    print(f"Batch created with ID {batch.id}.")
    return batch.id


# â”€â”€â”€ Step 5: Poll for Completion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def poll_batch_status(batch_id):
    """
    Periodically check the status of the batch job until it finishes.
    Abstractly: wait for LLM work to complete â†’ handle failures â†’ return output handle.
    """
    status = None
    while status != "completed":
        batch = client.batches.retrieve(batch_id)
        status = batch.status
        print(f"Batch {batch_id} status: {status}")
        if status in ("failed", "cancelled"):
            raise Exception(f"Batch {batch_id} did not complete: {status}")
        time.sleep(60)  # back off between polls
    print(f"Batch {batch_id} completed.")
    return batch.output_file_id


# â”€â”€â”€ Step 6: Fetch Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def retrieve_batch_results(output_file_id, output_file_name="batch_output.jsonl"):
    """
    Download the completed batchâ€™s results as JSONL.
    Abstractly: pull LLM responses back to local storage for post-processing.
    """
    result = client.files.content(output_file_id)
    with open(output_file_name, 'w', encoding='utf-8') as f:
        f.write(result.text)
    print(f"Results saved to '{output_file_name}'.")


# â”€â”€â”€ Step 7: Postâ€Process & Integrate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_batch_results(output_file_name="batch_output.jsonl"):
    """
    Read each JSONL line from the batch output, parse the JSON string returned
    by the model, and return a list of tuples:
      (entry_id, wazn, form)
    """
    results = []
    with open(output_file_name, 'r', encoding='utf-8') as f:
        for line in f:
            record = json.loads(line)
            try:
                content_json = record["response"]["body"]["choices"][0]["message"]["content"].strip()
                parsed = json.loads(content_json)
                entry_id = parsed["id"]
                wazn     = parsed["wazn"]
                form     = parsed["form"]
                results.append((entry_id, wazn, form))
                print(f"{entry_id:10} | {wazn:20} | {form}")
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Skipping malformed line: {e}")
    return results

def main():
    print("Options:\n1. Process existing results\n2. Run full pipeline (resumable)")
    choice = input("Select 1 or 2: ").strip()

    words_data = read_csv("clean_defs.csv", limit_definition_length=500)
    batch_size = 1000
    total = len(words_data)

    input_batches, output_batches = get_batch_indexes()

    if choice == "1":
        # This can loop through all batch_output files if needed
        for batch_num in sorted(output_batches):
            output_name = f"batch_output_{batch_num}.jsonl"
            process_batch_results(output_file_name=output_name)

    elif choice == "2":
        for start in range(0, total, batch_size):
            batch_index = (start // batch_size) + 1
            input_file = f"batchinput_{batch_index}.jsonl"
            output_file = f"batch_output_{batch_index}.jsonl"

            if batch_index in output_batches:
                print(f"âœ”ï¸ Skipping batch {batch_index} (already retrieved)")
                continue

            # If the input file doesn't exist, create it
            if batch_index not in input_batches:
                print(f"ğŸ“ Creating new batch input file {input_file}")
                create_batch_input_file(words_data, file_name=input_file, start_idx=start, batch_size=batch_size)
            else:
                print(f"â†ªï¸ Reusing existing input file {input_file}")

            # Send + retrieve output
            file_id = upload_batch_file(file_name=input_file)
            batch_id = create_batch(file_id)
            out_id = poll_batch_status(batch_id)
            retrieve_batch_results(out_id, output_file_name=output_file)
            process_batch_results(output_file_name=output_file)

    else:
        print("Invalid choice. Exiting.")
        
if __name__ == "__main__":
    main()